{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VIGyIus8Vr7"
   },
   "source": [
    "Take a look at the [repository](https://github.com/davexhardware/CycleGAN-VAE-NST) for more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7wNjDKdQy35h"
   },
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always remember to pull the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ePlDtzfqNPe4",
    "outputId": "b9c0b533-5958-402f-dae6-8d3d3b9ca992"
   },
   "outputs": [],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed, create a virtual environment for the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Define the name of the virtual environment\n",
    "venv_name = 'venv'\n",
    "\n",
    "# Create the virtual environment\n",
    "os.system(f'{sys.executable} -m venv {venv_name}')\n",
    "\n",
    "print(f'Virtual environment \"{venv_name}\" created successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the required packages to the virtual environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "z1EySlOXwwoa",
    "outputId": "dbdbcf7b-2195-4a54-c2b1-241601a4a72c",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8daqlgVhw29P"
   },
   "source": [
    "# Datasets\n",
    "\n",
    "Use your own dataset by creating the appropriate folders and adding in the images.\n",
    "\n",
    "-   Create a dataset folder under `/dataset` for your dataset.\n",
    "-   We will create subfolders `testA`, `testB`, `trainA`, and `trainB` under your dataset's folder. For our experiment, we're going to extract the tensors from the images available in online datasets, the <a href=\"https://www.kaggle.com/datasets/jessicali9530/celeba-dataset\">Celeba dataset</a> and the <a href=\"https://www.kaggle.com/datasets/ibrahimserouis99/one-piece-image-classifier\">>Onepiece Dataset</a> you can download with:\n",
    "\n",
    "\\*\\***Notice that you have to change folders before running those scripts**\\*\\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rm1cjvqnkd99",
    "outputId": "f21883f3-bd64-47c7-d1be-3687887dbcd1",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%python ./data/custom_dataset_nst/prepare_celeba.py\n",
    "%python ./data/custom_dataset_nst/prepare_onepiece.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will download and flatten the datasets, but the Onepiece contains a lot of images showing actions, movements and scenes that must be excluded or cropped. In order to do this we used a simple Computer Vision classic method, the *Haarcascades*, but we also reference a more complex approach, based on a pretrained Face Recognition DL model. You can check them in [crop_onepiece.py](./data/custom_dataset_nst/crop_onepiece.py) and [crop_onepiece_deep_.py](./data/custom_dataset_nst/crop_onepiece_deep.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%python ./data/custom_dataset_nst/crop_onepiece.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can convert the images to tensors using this script, that will use the default DataLoader with some additional features to convert and store images in the folders you will specify inside tensors in new folders that will be renamed adding '_pt' at the end.\n",
    "\n",
    "It will also resize the image to a squared \\<LOAD_SIZE\\>px and crop to \\<CROP\\>px.\n",
    "\n",
    "\\*\\***Also here you will have to change directories and check the command usage**\\*\\*\n",
    "\n",
    "Change gpu_ids value if you want to run it on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%python ./iterate_dataset.py --transform_float16 --preprocess resize_and_crop --dataroot ./datasets --gpu_ids <GPU_ID> --load_size <LOAD_SIZE> --crop_size <CROP>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can pick up the number of tensors that you want and move them to the training folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yNw37Fd9LaGu",
    "outputId": "49ae8e70-d052-43b5-eb2a-ee9dd78a7150"
   },
   "outputs": [],
   "source": [
    "#I want to pick N random files from source_dir and move them to dest_dir\n",
    "\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "def move_files(dset):\n",
    "    files = os.listdir(dset['source_dir'])\n",
    "\n",
    "    if len(files) < dset['num_files_to_move']:\n",
    "        print(f\"Warning: Only {len(files)} files found in {dset['source_dir']}. Moving all of them.\")\n",
    "        num_files_to_move = len(files)\n",
    "    else:\n",
    "        num_files_to_move = dset['num_files_to_move']\n",
    "    \n",
    "    random_files = random.sample(files, num_files_to_move)\n",
    "\n",
    "    # Move the files to the destination directory\n",
    "    for file in random_files:\n",
    "        source_path = os.path.join(dset['source_dir'], file)\n",
    "        dest_path = os.path.join(dset['dest_dir'], file)\n",
    "        shutil.copy(source_path, dest_path)\n",
    "\n",
    "    print(f\"Copied {num_files_to_move} random files from {dset['source_dir']} to {dset['dest_dir']}\")\n",
    "\n",
    "\n",
    "# Define source and destination directories\n",
    "dirs=[\n",
    "    {\n",
    "        'source_dir' : './datasets/img_align_celeba_pt',\n",
    "        'dest_dir' : './datasets/trainA',\n",
    "        'num_files_to_move' : 2000\n",
    "    },\n",
    "    {\n",
    "        'source_dir' : './datasets/onepiece_pt',\n",
    "        'dest_dir' : './datasets/trainB', \n",
    "        'num_files_to_move' : 2000\n",
    "    }\n",
    "]\n",
    "\n",
    "# Ensure that the source directories exist\n",
    "for el in dirs:\n",
    "    if not os.path.exists(el['source_dir']):\n",
    "        raise Exception(f\"Source directory {el['source_dir']} doesn't exist\")\n",
    "\n",
    "\n",
    "# Create the destination directories if they don't exist and move the files\n",
    "for el in dirs:\n",
    "    os.makedirs(el['dest_dir'], exist_ok=True)\n",
    "    move_files(el)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to verify the effective sizes of the set, use this (eventually add test folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T09:04:26.323140Z",
     "start_time": "2025-03-05T09:04:26.087884Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q8QMGYp8JhRs",
    "outputId": "b0bb79a0-ebbc-445f-ba64-4bab2eef1b1a",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "!ls ./datasets/trainB  | wc -l\n",
    "!ls ./datasets/trainA  | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFw1kDQBx3LN"
   },
   "source": [
    "# Training\n",
    "\n",
    "Ensure that CUDA is available on your machine, otherwise you should run the model on the CPU or download the necessary CUDA drivers and compatible <a href=\"https://pytorch.org/\">PyTorch</a> version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T09:34:39.510567Z",
     "start_time": "2025-03-05T09:34:38.326649Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0j5AkG-ny53H",
    "outputId": "62a30881-0ccc-48c6-b098-d770ad7ea055"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"cuDNN enabled:\", torch.backends.cudnn.enabled)\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version() if torch.backends.cudnn.is_available() else \"Not available\")\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")\n",
    "\n",
    "x = torch.randn(1, 1, 32, 32, device=\"cuda\")  # Random tensor on GPU\n",
    "conv = torch.nn.Conv2d(1, 1, kernel_size=3).cuda()  # Simple Conv2D layer\n",
    "with torch.backends.cudnn.flags(enabled=True):\n",
    "    output = conv(x)\n",
    "print(\"cuDNN acceleration is working!\" if torch.backends.cudnn.enabled else \"cuDNN is not being used.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now it's time to train the model. In order to set up the correct parameters, please take a closer look at the [options](./options/base_options.py) files and to the [tips](./docs/tips.md)\n",
    "\n",
    "If you have a GUI and/or an access to the ports of the machine you are training on, you may enjoy looking at some training loss curves and intermediate results on the visdom-provided dashboard. \n",
    "First, remove  `--display_id -1`  from the next command, and when you start it the script will connect automatically, if  `python -m visdom.server` is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0sp7TCT2x9dB",
    "outputId": "e1e39216-484a-49d7-e8bb-84d1935791fc",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%python train.py --dataroot ./datasets --name portraits2op --lambda_identity 0 --latent_dim 256 --init_type xavier --norm bn --netG ResnetKVAE --dataset_mode tensor --crop_size 128 --batch_size 16 --verbose --n_epochs 50 --n_epochs_decay 50 --display_id -1 --beta1 0.65 --lambda_A 9.0 --lambda_kl 1.1 --lr 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize\n",
    "\n",
    "In case you don't have the possibility to use visdom and you want to manually check the results during training epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%ls ./checkpoints/portraits2op/web/images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = plt.imread('./checkpoints/portraits2op/web/images/epoch002_fake_B.png')\n",
    "plt.imshow(img)\n",
    "\n",
    "img = plt.imread('./checkpoints/portraits2op/web/images/epoch002_real_A.png')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrlIppjgJhR0"
   },
   "source": [
    "### While training\n",
    "In case you have huge models or architectures and limited disk space, use the following script to clear the older checkpoints of the model. You can run it also during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9YE2HcPJhR1"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "checkpoints_dir='./checkpoints/portraits2op'\n",
    "\n",
    "def remove_files_with_regex(directory, pattern):\n",
    "    for filename in os.listdir(directory):\n",
    "        if re.match(pattern, filename):\n",
    "            os.remove(os.path.join(directory, filename))\n",
    "\n",
    "pattern = r'[0-9]+\\_net\\_[GD]\\_[AB]\\.pth'\n",
    "remove_files_with_regex(checkpoints_dir, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9UkcaFZiyASl"
   },
   "source": [
    "# Testing\n",
    "\n",
    "Once your model has trained, copy over the last checkpoint to a format that the testing model can automatically detect:\n",
    "\n",
    "Use:\n",
    "- `cp ./checkpoints/<MODELNAME>/latest_net_G_A.pth ./checkpoints/<MODELNAME>/latest_net_G.pth` if you want to transform images from class A to class B \n",
    "- or`cp ./checkpoints/<MODELNAME>/latest_net_G_B.pth ./checkpoints/<MODELNAME>/latest_net_G.pth` if you want to transform images from class B to class A.\n",
    "\n",
    "In case you want to test models at different epochs, create a new *cp_test* folder and one subfolder for each training epoch you want to test, e.g. `cp ./checkpoints/real2onepiece_portraits_pt/250_net_G_A.pth ./cp_test/real2op250/latest_net_G.pth`\n",
    "Testing here is done using images, so you'll have to load them in the testA folder.\n",
    "If you want to pick a defined number of images from the train starting folder to the test one, run this script:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T19:05:35.703614Z",
     "start_time": "2025-03-11T19:05:35.626116Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "def move_to_test(img_dir, test_dir, num_files):\n",
    "  if os.path.exists(img_dir):\n",
    "    images = [f for f in os.listdir(img_dir) if os.path.isfile(os.path.join(img_dir, f))]\n",
    "    random.shuffle(images)\n",
    "    images_to_move = images[:num_files]\n",
    "    if not os.path.exists(test_dir):\n",
    "        os.mkdir(test_dir)\n",
    "    for image in images_to_move:\n",
    "        if '_fake' in image:\n",
    "            shutil.copy(os.path.join(img_dir, image), os.path.join(test_dir, image.replace('_fake','')))\n",
    "  else:\n",
    "    print(f\"Directory {img_dir} does not exist\")\n",
    "\n",
    "path= './results/inverse_results/real2op200_reference_idt_full_B_to_A/test_latest/images'\n",
    "pathdst = './datasets/test_fakeportr2op/testA'\n",
    "test_size = 100\n",
    "move_to_test(path, pathdst, test_size)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to let the model generate the images, run:\n",
    "\n",
    "-   `python test.py --dataroot datasets/<EXPERIMENT>/testA --name <MODEL_NAME> --model test --no_dropout`\n",
    "\n",
    "Change the `--dataroot` and `--name` to be consistent with your trained model's configuration.\n",
    "\n",
    "> from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix:\n",
    "> The option --model test is used for generating results of CycleGAN only for one side. This option will automatically set --dataset_mode single, which only loads the images from one set. On the contrary, using --model cycle_gan requires loading and generating results in both directions, which is sometimes unnecessary. The results will be saved at ./results/. Use --results_dir {directory_path_to_save_result} to specify the results directory.\n",
    "\n",
    "**For your own experiments, you might want to specify --netG, --norm, --no_dropout to match the generator architecture of the trained model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCsKkEq0yGh0"
   },
   "outputs": [],
   "source": [
    "!python test.py --dataroot datasets/horse2zebra/testA --name horse2zebra_pretrained --model test --no_dropout"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!python test.py --dataroot ./datasets/test_real2op --checkpoints_dir ./cp_test --name real2op200_reference_idt_full_B_to_A --crop_size 128 --load_size 140 --netG resnet_9blocks --no_dropout --norm batch"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "CycleGAN",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
